{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651e4f56",
   "metadata": {},
   "source": [
    "# üß† NLP Foundations Workshop: Vector Space Proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ddb017",
   "metadata": {},
   "source": [
    "### üîπ Introduction to Vector Space Proximity\n",
    "\n",
    "A large majority of the data on the Internet is **unstructured**, for example: social media posts, emails, images, videos and audio files.\n",
    "\n",
    "If we want to **persist** all these media in a database, we may add **metadata** about them, such as file type or creation date timestamp, or we could  **tag** each file, or parts of it, so they are easy to search for. This is because it would be very difficult to identify them based on their low-level (byte) representations.\n",
    "\n",
    "But, what if we want to make the process fully automated (i.e., remove the need to manually add features, like tags, to each media item)? We need another way to represent the semantics of digital media.\n",
    "\n",
    "That is the reason why in **Information Retrieval (IR)** and **Natural Language Processing (NLP)**, we often represent documents and queries as **vectors** in a **high-dimensional space**, where:\n",
    "\n",
    "* Each **dimension** corresponds to a **unique term** in the vocabulary.\n",
    "* A **document** is represented by a **point** or a **vector** in the space.\n",
    "* A **vector** is a list of weights (e.g., term frequencies, TF-IDF values) that describe the presence or importance of terms in a document or query.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Example 1: \"Rich\" and \"Poor\" Axes\n",
    "\n",
    "![Vector Space Example: \"Rich\" and \"Poor\" Axes\"](./images/Fig1_CartesianVectorSpace.png)\n",
    "\n",
    "Suppose our vocabulary only has two terms:\n",
    "\n",
    "* `\"rich\"`\n",
    "* `\"poor\"`\n",
    "\n",
    "These two terms define a **2D Cartesian space**:\n",
    "\n",
    "* The **x-axis** corresponds to the term **\"rich\"**.\n",
    "* The **y-axis** corresponds to the term **\"poor\"**.\n",
    "\n",
    "Each document is represented as a vector in this space:\n",
    "\n",
    "* A document with many occurrences of ‚Äúpoor‚Äù and none of ‚Äúrich‚Äù lies near the **y-axis**.\n",
    "* A document that mentions both ‚Äúrich‚Äù and ‚Äúpoor‚Äù lies in the **first quadrant**.\n",
    "* A document with only ‚Äúrich‚Äù is aligned along the **x-axis**.\n",
    "\n",
    "The **query vector** $q = \\{\\text{\"rich\"}, \\text{\"poor\"}\\}$ points in the direction of interest for the search engine.\n",
    "\n",
    "### üîπ Euclidean Distance and Its Limitations\n",
    "\n",
    "One might assume we can measure similarity using **Euclidean distance**:\n",
    "\n",
    "$$\n",
    "\\text{Euclidean}( \\vec{q}, \\vec{d} ) = \\sqrt{ \\sum_{i=1}^{n} (q_i - d_i)^2 }\n",
    "$$\n",
    "\n",
    "However, this has problems in practice:\n",
    "\n",
    "* If document $d_2$ contains more occurrences of both ‚Äúrich‚Äù and ‚Äúpoor‚Äù than the query, its vector will have a **longer length**.\n",
    "* As seen in the diagram, even though $d_2$ has strong content overlap with the query $q$, it may still be **further away** in Euclidean terms than unrelated documents like $d_3$.\n",
    "* This happens because **magnitude dominates**, not direction.\n",
    "\n",
    "### üîπ Angle as Similarity ‚Üí Cosine Similarity\n",
    "\n",
    "To solve this, we focus on **vector direction**, not length. We measure **angle** between the document and query vectors using **Cosine Similarity**:\n",
    "\n",
    "$$\n",
    "\\cos(\\vec{q}, \\vec{d}) = \\frac{ \\vec{q} \\cdot \\vec{d} }{ \\|\\vec{q}\\| \\cdot \\|\\vec{d}\\| }\n",
    "= \\frac{ \\sum_{i=1}^{n} q_i \\cdot d_i }{ \\sqrt{ \\sum_{i=1}^{n} q_i^2 } \\cdot \\sqrt{ \\sum_{i=1}^{n} d_i^2 } }\n",
    "$$\n",
    "\n",
    "* This gives us a similarity score from **0 (orthogonal)** to **1 (identical direction)**.\n",
    "* Longer documents that are semantically aligned still get **high similarity**.\n",
    "\n",
    "### üîπ Why Cosine Similarity Works Better\n",
    "\n",
    "* **Angle** captures **semantic alignment**.\n",
    "* It is **not affected** by document length or repetition.\n",
    "* Example: duplicating document $d$ to make $d'$ will increase Euclidean distance ‚Äî but **cosine similarity remains 1**.\n",
    "\n",
    "Cosine similarity is at the core of:\n",
    "\n",
    "* **Search ranking**\n",
    "* **Embedding-based retrieval**\n",
    "* **LLM scoring and attention mechanisms**\n",
    "\n",
    "Sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc3dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['rich poor']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Document",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Cosine Similarity with Query",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "259dc1a3-5d4a-434b-9108-cdf3101a0ad8",
       "rows": [
        [
         "0",
         "Doc2",
         "0.7071067811865475"
        ],
        [
         "1",
         "Doc1",
         "0.0"
        ],
        [
         "2",
         "Doc3",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Cosine Similarity with Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doc2</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doc1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doc3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document  Cosine Similarity with Query\n",
       "0     Doc2                      0.707107\n",
       "1     Doc1                      0.000000\n",
       "2     Doc3                      0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the documents and the query\n",
    "documents = [\n",
    "    \"Ranks of starving poets swell\",       # d1\n",
    "    \"Rich poor gap grows\",                 # d2\n",
    "    \"Record baseball salaries in 2010\"     # d3\n",
    "]\n",
    "\n",
    "query = [\"rich poor\"]                     # q\n",
    "\n",
    "# Create a CountVectorizer to convert text to term frequency vectors\n",
    "vectorizer = CountVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(documents + query).toarray()\n",
    "\n",
    "# Separate vectors\n",
    "doc_matrix = doc_vectors[:3]  # d1, d2, d3\n",
    "query_vector = doc_vectors[3].reshape(1, -1)  # q\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarities = cosine_similarity(query_vector, doc_matrix).flatten()\n",
    "\n",
    "# Create a DataFrame to show results\n",
    "df = pd.DataFrame({\n",
    "    'Document': ['Doc1', 'Doc2', 'Doc3'],\n",
    "    'Cosine Similarity with Query': cosine_similarities\n",
    "})\n",
    "\n",
    "# Sort for clarity\n",
    "print(\"Query: \", query)\n",
    "df.sort_values(by='Cosine Similarity with Query', ascending=False, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25563bf8",
   "metadata": {},
   "source": [
    "### üìò Example 2: Word Vectors in a Small Corpus\n",
    "\n",
    "Let's start with a small corpus of just six words, each represented by a vector in 3D space:\n",
    "\n",
    "```plaintext\n",
    "CAT     ‚Üí [ 0.2, -0.4,  0.7]\n",
    "DOG     ‚Üí [ 0.6,  0.1,  0.5]\n",
    "APPLE   ‚Üí [ 0.8, -0.2, -0.3]\n",
    "ORANGE  ‚Üí [ 0.7, -0.1, -0.6]\n",
    "HAPPY   ‚Üí [-0.5,  0.9,  0.2]\n",
    "SAD     ‚Üí [ 0.4, -0.7, -0.5]\n",
    "```\n",
    "\n",
    "Each term is represented by a **vector in 3D space**.\n",
    "\n",
    "### üîç Observations\n",
    "\n",
    "- Words with **similar meanings** tend to have **similar vector representations**.\n",
    "  - For example, **APPLE** and **ORANGE** are close in vector space, reflecting their semantic similarity.\n",
    "\n",
    "- Words with **opposite meanings** tend to have **vectors pointing in opposite directions**.\n",
    "  - For instance, **HAPPY** and **SAD** have contrasting vectors, indicating their opposing emotional tones.\n",
    "\n",
    "![3D Visualizationof Word Vectors](./images/Fig2_3DVisualizationWordVectors.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8a0ea",
   "metadata": {},
   "source": [
    "Vector representations are also called **Embeddings**.\n",
    "\n",
    "There are several approaaches to how **word embedding methods** generate effective vector representations. \n",
    "\n",
    "One of them is **frequency-based embeddings**, word representations that are derived from the frequency of words in a corpus. They are based on the idea that the **importance** or the **significance** of a word can be inferred from **how frequently it occurs in the text**. One such embedding is called **Term Frequency - Inverse Document Frequency** or **TF-IDF**. \n",
    "\n",
    "TF-IDF highlights words that are frequent within a specific document but are rare across the entire corpus. For example, in a document about music, it would emphasize words such as **rap**, **disco**, **pop**, **rock**. On the other hand, pronouns would receive a low TF-IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8f764",
   "metadata": {},
   "source": [
    "There are various models for generating word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42c1ba",
   "metadata": {},
   "source": [
    "### üîπ Curriculum Learning (9): Word Embeddings with Word2Vec\n",
    "\n",
    "**Word2Vec** is one of the most influential models for learning **dense vector representations** of words, also known as **embeddings**.\n",
    "\n",
    "Unlike frequency-based models like TF-IDF, Word2Vec uses a **neural network** to learn word vectors such that **similar words have similar embeddings**.\n",
    "\n",
    "There are two main architectures:\n",
    "\n",
    "* **CBOW (Continuous Bag of Words)**: Predicts a word from its context.\n",
    "* **Skip-gram**: Predicts context words from a target word.\n",
    "\n",
    "Both approaches rely on the **distributional hypothesis**: words that appear in similar contexts tend to have similar meanings.\n",
    "\n",
    "### üíª Code Challenge: Learn Word Embeddings Using Word2Vec\n",
    "\n",
    "#### üöÄ Your Task:\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "1. Prepares a small corpus of tokenized sentences.\n",
    "2. Trains a **Word2Vec** model on this corpus using Gensim.\n",
    "3. Displays the vector representation for a few words.\n",
    "4. Finds the most similar words to a chosen term.\n",
    "\n",
    "#### üìö Hints:\n",
    "\n",
    "* Use `from gensim.models import Word2Vec`\n",
    "* Tokenize your corpus as a list of word lists (sentences).\n",
    "* Try: `model.wv['word']`, `model.wv.most_similar('word')`\n",
    "\n",
    "**Example Questions to Explore:**\n",
    "\n",
    "* What is the shape of a word vector?\n",
    "* Which words are closest to \"learning\", \"data\", or \"model\"?\n",
    "* Can Word2Vec capture analogies (e.g., \"king\" - \"man\" + \"woman\")?\n",
    "\n",
    "Try it out and see what your model learns! üéØ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e3348",
   "metadata": {},
   "source": [
    "Use cases:\n",
    "\n",
    "- Long term memory for LLMs.\n",
    "- Semantic Search: based on the meaning or context.\n",
    "- Similarity search for text, images, audio or video data.\n",
    "- Ranking and/or recommendation engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9ac23",
   "metadata": {},
   "source": [
    "LangChain\n",
    "\n",
    "LangChain is a framework for developing application powered by Large Language Models (LLMs). \n",
    "It was designed and implemented to be:\n",
    "- Data-aware: connecting a language model to other sources of data.\n",
    "- Agentic: allowing a model to interact with its environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
